# 机器学习

## 一、介绍

### 1、机器学习介绍

生物的本能——人设定好的

后天学习——机器学习

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/1%E3%80%81turn%20off%E7%A4%BA%E4%BE%8B.png?raw=true" alt="img" style="zoom:67%;" />

听到tun off就关闭，但是如果说don't turn off同样会关闭。

Machine Learning ≈ Looking for a Function ==**From Data**==

### 2、机器学习相关技术

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/2%E3%80%81learning%20map.png?raw=true" alt="img" style="zoom:67%;" />

#### 1)Supervised  Learning 监督学习

##### (1)Regression 回归

Regression是一种machine learning的task

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/3%E3%80%81regression.png?raw=true" alt="img" style="zoom:67%;" />

##### (2)Classification 分类

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/4%E3%80%81Classification.png?raw=true" alt="img" style="zoom:67%;" />

Binary Classification二分类 

Multi-class Classification 多分类

Deep Learning ：Image Recognition 图像识别、Playing GO

SVM、decision tree、K-NN...

以上都是supervised learning（监督学习），监督学习的问题是我们需要大量的training data。training data告诉我们要找的function的input和output之间的关系。如果我们在监督学习下进行学习，我们需要告诉机器function的input和output是什么。这个output往往没有办法用很自然的方式取得，需要人工的力量把它标注出来，这些function的output叫做label。

#### 3）Semi-supervised Learning 半监督学习

减少label需要的量

#### 4）Transfer Learning 迁移学习

减少label需要的量

#### 5）Unsupervised Learning 无监督学习

#### 6）Structured Learning 监督学习中的结构化学习

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/5%E3%80%81Structured%20Learning.png?raw=true" alt="img" style="zoom:67%;" />

#### 7）Reinforcement Learning 强化学习

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/day01/6%E3%80%81Reinforcement%20Learning.png?raw=true" alt="img" style="zoom:67%;" />

在reinforcement learning里面，我们没有告诉机器正确的答案是什么，机器所拥有的只有一个分数，就是他做的好还是不好。

# 二、回归

## 1、回归的定义

Regression 就是找到一个函数 function，通过输入特征 x，输出一个数值 Scalar。

## 2、应用举例

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/1%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%E3%80%81.png?raw=true" alt="img" style="zoom:67%;" />

## 3、模型步骤

- step1：模型假设，选择模型框架（线性模型）
- step2：模型评估，如何判断众多模型的好坏（损失函数）
- step3：模型优化，如何筛选最优的模型（梯度下降）

### 1) Step 1：模型假设 - 线性模型

#### (1) 一元线性模型（单个特征）

以一个特征$x_{cp}$为例，线性模型假设$y=b+wx_{cp}$

#### (2) 多元线性模型（多个特征）

在实际应用中，输入特征肯定不止$x_{cp}$这一个。

所以我们假设线性模型Linear model：$y=b+\sum{w_i}{x_i}$。

$x_i$：各种特征(feature)$x_{cp},x_{hp},x_w,x_h...$

$w_i$：各个特征的权重$w_{cp},w_{hp},w_w,w_h...$

$b$：偏移量

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/2%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.png?raw=true" alt="img" style="zoom:67%;" />

### 2）Step 2：模型评估 - 损失函数

收集和查看训练数据

【单个特征】$x_{cp}$

定义$x^1$是输入值，$\hat{y}^1$是输出值，$\hat{}$所代表的是真实值。

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/3%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.png?raw=true" alt="img" style="zoom:67%;" />

如何判断众多模型的好坏：Loss function 损失函数

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/4%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png?raw=true" alt="img" style="zoom:67%;" />

实际值-模型估计值（公式见图）

我们将w,b在二维坐标图中展示，如图所示：

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/5%E4%BA%8C%E7%BB%B4%E5%9D%90%E6%A0%87%E5%9B%BE%E6%98%BE%E7%A4%BA.png?raw=true" alt="img" style="zoom:67%;" />

- 图中每一个点代表着一个模型对应的 w 和 b
- 颜色越深代表模型更优

### 3）Step 3：最佳模型 - 梯度下降

【单个特征】$x_{cp}$

如何筛选最优的模型（参数w,b）

已知损失函数，需要找到一个令结果最小的$f^*$

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/6%E6%89%BEf.png?raw=true" alt="img" style="zoom:67%;" />

注：$arg\,\max\limits_x\,f(x)$：当f(x)取最大值时，x的取值

​		$arg\,\min\limits_x\,f(x)$：当f(x)取最小值时，x的取值

**梯度下降** 

先从最简单的只有一个参数w入手，定义$w^* = arg\,\min\limits_f\,L(w)$

引入一个概念 学习率 ：移动的步长，如图中 $\eta$

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/7%E6%AD%A5%E9%95%BF.png?raw=true" alt="img" style="zoom:67%;" />

- 步骤1：随机选取一个 $w^0$
- 步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向

​        小于0向右移动（增加w）

​        大于0向左移动（减少w）

- 步骤3：根据学习率移动
- 重复步骤2和步骤3，直到找到最低点

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/8%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F.png?raw=true" alt="img" style="zoom:67%;" />

我们有可能会找到当前的最小值，并不是全局的最小值。

2个模型参数 w 和 b，过程是类似的，需要做的是偏微分

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/9%E5%81%8F%E5%BE%AE%E5%88%86.png?raw=true" alt="img" style="zoom:67%;" />

整理成更简单的公式（见上图右上角）

把 w 和 b在图形中展示：

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/10w%E5%92%8Cb%E5%9B%BE.png?raw=true" alt="img" style="zoom:67%;" />

- 每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小
- 红色的箭头代表等高线的法线方向

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/11%E7%AD%89%E9%AB%98%E7%BA%BF.png?raw=true" alt="img" style="zoom:67%;" />

worry? don't worry，没有局部最优（损失函数是凸的）

对w和b求偏微分

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/12%E6%B1%82%E5%81%8F%E5%BE%AE%E5%88%86.png?raw=true" alt="img" style="zoom:67%;" />

## 4、如何验证训练好的模型的好坏

使用训练集和测试集的平均误差来验证模型的好坏

## 5、更强大复杂的模型：1元N次线性模型

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/13%E4%B8%80%E5%85%83N%E6%AC%A1%E6%A8%A1%E5%9E%8B.png?raw=true" alt="img" style="zoom:67%;" />

高次的包含低次

## 6、过拟合

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/14%E8%BF%87%E6%8B%9F%E5%90%88.png?raw=true" alt="img" style="zoom:67%;" />

更复杂的模型不一定更好，选择最适合的model

## 7、步骤优化

### 1）Step1优化：模型合并

2个input的四个线性模型合并到一个线性模型中

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/15%E5%90%88%E5%B9%B6%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.png?raw=true" alt="img" style="zoom:67%;" />

改写为linear model

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/16%E6%94%B9%E5%86%99%E4%B8%BAlinear%20model.png?raw=true" alt="img" style="zoom:67%;" />

（效果拔群QAQ）

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/17%E6%95%88%E6%9E%9C%E6%8B%94%E7%BE%A4.png?raw=true" alt="img" style="zoom:67%;" />

### 2）Step2优化：更多参数，更多input

如果希望模型更强大表现更好（更多参数，更多input）

考虑血量、身高、体重等更多特征

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/18%E6%9B%B4%E5%A4%9A%E5%8F%82%E6%95%B0%EF%BC%8C%E6%9B%B4%E5%A4%9Ainput.png?raw=true" alt="img" style="zoom:67%;" >

overfitting！

### 3）Step3优化：加入正则化

更多特征，但是权重 w 可能会使某些特征权值过高，仍旧导致overfitting，所以加入正则化

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/19%E6%AD%A3%E5%88%99%E5%8C%96.png?raw=true" alt="img" style="zoom:67%;" >

w 越小，表示 function 较平滑的

如果有噪声，平滑的function受到的影响更小。

<img src="https://github.com/BobWang4/DataWhale_learnNotes/blob/main/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/task02/20%E5%B9%B3%E6%BB%91.png?raw=true" alt="img" style="zoom:67%;" >

并不是 w越小模型越平滑越好；

b对曲线平滑是没有影响

## 8、总结

Gradient descent：梯度下降

Overfitting和Regularization：过拟合和正则化
